# Apache Airflow ETL Pipeline

A production-ready ETL pipeline built with Apache Airflow that processes real-time data from Kafka, cleans and transforms it, and loads it into Snowflake for analytics.

## ğŸš€ Overview

This project implements a complete ETL (Extract, Transform, Load) pipeline using Apache Airflow 3.1.3. The pipeline consumes event data from Kafka topics, performs data cleaning and transformation, and loads the processed data into Snowflake for business intelligence and analytics.

## ğŸ“‹ Features

- **Real-time Data Processing**: Polls Kafka topics for new messages every minute
- **Data Cleaning & Transformation**: Automated data cleaning and normalization
- **Snowflake Integration**: Seamless data loading to Snowflake data warehouse
- **Docker-based Deployment**: Containerized setup for easy deployment and scaling
- **Modular Architecture**: Reusable utility modules for common operations
- **Error Handling**: Robust error handling with retry mechanisms
- **Comprehensive Logging**: Detailed logging for monitoring and debugging

## ğŸ—ï¸ Architecture

The pipeline consists of three main DAGs that work together:

```
Kafka Topic â†’ polls_dag â†’ clean_dag â†’ commit_dag â†’ Snowflake
```

### Pipeline Flow

1. **polls_dag**: Polls Kafka topic for new messages and saves raw JSON files
2. **clean_dag**: Cleans and transforms raw data into structured CSV format
3. **commit_dag**: Loads cleaned data into Snowflake data warehouse

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dags/                      # Airflow DAG definitions
â”‚   â”œâ”€â”€ polls_dag.py          # Kafka polling DAG
â”‚   â”œâ”€â”€ clean_dag.py          # Data cleaning DAG
â”‚   â”œâ”€â”€ commit_dag.py         # Snowflake loading DAG
â”‚   â”œâ”€â”€ config.py             # Centralized configuration
â”‚   â””â”€â”€ utils/                # Utility modules
â”‚       â”œâ”€â”€ file_utils.py     # File operations
â”‚       â”œâ”€â”€ kafka_utils.py    # Kafka consumer functions
â”‚       â””â”€â”€ data_utils.py     # Data cleaning functions
â”œâ”€â”€ scripts/                   # Helper scripts
â”‚   â”œâ”€â”€ kafka/                # Kafka producer/consumer scripts
â”‚   â””â”€â”€ setup_snowflake_connection.py
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ airflow.cfg           # Airflow configuration
â”‚   â””â”€â”€ kafka/                # Kafka configuration
â”œâ”€â”€ data/                      # Data directories
â”‚   â”œâ”€â”€ raw/                  # Raw JSON files from Kafka
â”‚   â””â”€â”€ cleaned/              # Cleaned CSV files
â”œâ”€â”€ docker-compose.yaml        # Docker Compose configuration
â”œâ”€â”€ Dockerfile                # Custom Airflow image
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ GCP_DEPLOYMENT_PLAN.md   # GCP deployment guide
```

## ğŸ› ï¸ Technology Stack

- **Apache Airflow 3.1.3**: Workflow orchestration
- **Apache Kafka**: Real-time event streaming
- **Snowflake**: Cloud data warehouse
- **Docker & Docker Compose**: Containerization
- **Python 3.12**: Programming language
- **PostgreSQL**: Airflow metadata database
- **Redis**: Celery message broker

## ğŸ“¦ Dependencies

Key Python packages:
- `apache-airflow-providers-apache-kafka`: Kafka integration
- `apache-airflow-providers-snowflake`: Snowflake integration
- `kafka-python`: Kafka client library
- `confluent-kafka`: Confluent Kafka client
- `snowflake-connector-python`: Snowflake connector
- `pandas`: Data manipulation
- `python-dotenv`: Environment variable management

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose installed
- Kafka cluster accessible (or local Kafka setup)
- Snowflake account with appropriate permissions
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Deepak-583/airflow.git
   cd airflow
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

3. **Configure Kafka**
   - Place your Kafka `client.properties` file in `config/kafka/`
   - Ensure Kafka topic is accessible

4. **Configure Snowflake**
   - Set up Snowflake connection in Airflow UI (Admin â†’ Connections)
   - Connection ID: `snowflake_conn` (or configure in `config.py`)

5. **Start the services**
   ```bash
   docker-compose up -d
   ```

6. **Access Airflow UI**
   - Open http://localhost:8080
   - Default credentials: `airflow` / `airflow`

## âš™ï¸ Configuration

### Environment Variables

Key environment variables (set in `.env` file):

- `AIRFLOW_PROJ_DIR`: Base path for Airflow files (default: `/home/deepakbablu667/airflow`)
- `AIRFLOW_UID`: User ID for Airflow containers (default: `50000`)
- `KAFKA_TOPIC`: Kafka topic name (default: `event`)
- `KAFKA_POLL_TIMEOUT`: Poll timeout in seconds (default: `10`)
- `SNOWFLAKE_CONN_ID`: Airflow connection ID (default: `snowflake_conn`)
- `SNOWFLAKE_DATABASE`: Snowflake database (default: `ANALYTICS`)
- `SNOWFLAKE_SCHEMA`: Snowflake schema (default: `MY_SCHEMA`)
- `SNOWFLAKE_TABLE`: Snowflake table (default: `TEST`)

### DAG Configuration

All DAGs are configured in `dags/config.py` with support for environment variable overrides.

## ğŸ“Š DAGs Overview

### polls_dag
- **Schedule**: Every minute (`*/1 * * * *`)
- **Purpose**: Polls Kafka topic for new messages
- **Output**: Raw JSON files in `data/raw/`
- **Triggers**: `clean_dag` on successful completion

### clean_dag
- **Schedule**: Triggered by `polls_dag`
- **Purpose**: Cleans and transforms raw event data
- **Input**: Raw JSON files from `polls_dag`
- **Output**: Cleaned CSV files in `data/cleaned/`
- **Triggers**: `commit_dag` on successful completion

### commit_dag
- **Schedule**: Triggered by `clean_dag`
- **Purpose**: Loads cleaned data to Snowflake
- **Input**: Cleaned CSV files from `clean_dag`
- **Output**: Data in Snowflake table

## ğŸ”§ Development

### Running Locally

1. Ensure all services are running:
   ```bash
   docker-compose ps
   ```

2. Check DAGs in Airflow UI

3. Trigger DAGs manually or wait for scheduled runs

### Testing

Each DAG can be tested independently:
- **polls_dag**: Test Kafka connection and message consumption
- **clean_dag**: Test data cleaning with sample JSON files
- **commit_dag**: Test Snowflake connection and data loading

### Utility Scripts

- `scripts/kafka/producer.py`: Produce test messages to Kafka
- `scripts/kafka/consumer.py`: Consume messages from Kafka
- `scripts/setup_snowflake_connection.py`: Setup Snowflake connection

## â˜ï¸ Deployment

### GCP Deployment

See `GCP_DEPLOYMENT_PLAN.md` for detailed deployment instructions. Options include:

- **Cloud Composer**: Fully managed Airflow (recommended for production)
- **Google Kubernetes Engine (GKE)**: Full control with Kubernetes
- **Compute Engine (GCE)**: Simple VM-based deployment

### Production Considerations

- Use Cloud SQL for PostgreSQL instead of local postgres
- Use Cloud Memorystore for Redis instead of local redis
- Store DAGs in Cloud Storage
- Use Secret Manager for credentials
- Set up proper monitoring and alerting
- Configure backup strategies

## ğŸ“ Best Practices

1. **Separation of Concerns**: Business logic separated from DAG definitions
2. **Code Reusability**: Common functions in utility modules
3. **Configuration Management**: Centralized config with environment variable support
4. **Error Handling**: Proper exception handling and logging
5. **Documentation**: Docstrings and comments throughout
6. **Type Hints**: Type annotations for better code clarity
7. **Logging**: Structured logging at appropriate levels
8. **Retries**: Default retry configuration for all DAGs

## ğŸ› Troubleshooting

### Common Issues

1. **DAGs not appearing**: Check DAG folder permissions and Airflow logs
2. **Kafka connection errors**: Verify Kafka configuration and network connectivity
3. **Snowflake connection errors**: Check connection credentials in Airflow UI
4. **Permission errors**: Ensure proper file permissions for data directories

### Logs

View logs for specific services:
```bash
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler
docker-compose logs airflow-worker
```

## ğŸ“š Documentation

- [DAGs README](dags/README.md): Detailed DAG documentation
- [GCP Deployment Plan](GCP_DEPLOYMENT_PLAN.md): GCP deployment guide
- [Kafka Scripts README](scripts/kafka/README.md): Kafka utilities documentation

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

## ğŸ‘¤ Author

**Deepak Kalukuri**
- GitHub: [@Deepak-583](https://github.com/Deepak-583)

## ğŸ™ Acknowledgments

- Apache Airflow community
- Apache Kafka community
- Snowflake documentation

---

**Note**: This configuration is optimized for development. For production deployments, refer to the GCP deployment plan and follow security best practices.

